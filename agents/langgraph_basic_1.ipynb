{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karthikmanikandan/karthikmanikandan/blob/main/agents/langgraph_basic_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_core langchain_openai langchain_community langgraph google-search-results"
      ],
      "metadata": {
        "id": "7zeV62q2DiD6",
        "outputId": "f3015058-d7cf-40d6-dbfe-9f21e23e32db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "7zeV62q2DiD6",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-1.1.6-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (1.0.5)\n",
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (0.4.59)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (0.12.0)\n",
            "Collecting langchain_core\n",
            "  Downloading langchain_core-1.2.5-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (2.12.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.12.0)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain_community)\n",
            "  Downloading langchain_classic-1.0.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.45)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain_community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.13.2)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.12.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.0)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain_core) (3.0.0)\n",
            "Collecting langchain-text-splitters<2.0.0,>=1.1.0 (from langchain-classic<2.0.0,>=1.0.0->langchain_community)\n",
            "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph) (1.12.1)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2025.11.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2025.11.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.16.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_openai-1.1.6-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.2.5-py3-none-any.whl (484 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_classic-1.0.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
            "Downloading marshmallow-3.26.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32010 sha256=ff27fd54e68ac607844d41d027e2706b4624e2d63b01503c42255bcbc97341e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/47/f5/89b7e770ab2996baf8c910e7353d6391e373075a0ac213519e\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: requests, mypy-extensions, marshmallow, typing-inspect, google-search-results, dataclasses-json, langchain_core, langchain-text-splitters, langchain_openai, langchain-classic, langchain_community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: langchain_core\n",
            "    Found existing installation: langchain-core 1.2.1\n",
            "    Uninstalling langchain-core-1.2.1:\n",
            "      Successfully uninstalled langchain-core-1.2.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 google-search-results-2.4.2 langchain-classic-1.0.1 langchain-text-splitters-1.1.0 langchain_community-0.4.1 langchain_core-1.2.5 langchain_openai-1.1.6 marshmallow-3.26.2 mypy-extensions-1.1.0 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0200e82a",
      "metadata": {
        "id": "0200e82a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import operator\n",
        "from typing import TypedDict, Annotated, List\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.utilities import SerpAPIWrapper\n",
        "from langgraph.graph import StateGraph, END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "389f99ee",
      "metadata": {
        "id": "389f99ee"
      },
      "outputs": [],
      "source": [
        "os.environ[\"SERPAPI_API_KEY\"] = \"539ebe0258649e0c5d842301d523c0b0cffb7c5ff86f72977fb47832e3196f02\"\n",
        "\n",
        "llm = ChatOpenAI(openai_api_base = \"https://openrouter.ai/api/v1\", openai_api_key = \"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\", model = \"tngtech/deepseek-r1t2-chimera:free\",temperature=0.9) #max_tokens=100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2891b362",
      "metadata": {
        "id": "2891b362"
      },
      "outputs": [],
      "source": [
        "# --- Tool Definition ---\n",
        "# Instantiate the SerpAPI search tool.\n",
        "search_tool = SerpAPIWrapper()\n",
        "\n",
        "# --- State Definition ---\n",
        "# This defines the \"memory\" or \"state\" that flows through the graph.\n",
        "class ResearchState(TypedDict):\n",
        "    topic: str\n",
        "    explanation: str\n",
        "    summary: str\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6dc97a34",
      "metadata": {
        "id": "6dc97a34"
      },
      "outputs": [],
      "source": [
        "def researcher_agent(state: ResearchState) -> dict:\n",
        "    \"\"\"\n",
        "    This agent uses a web search tool to find information on a topic and then explains it.\n",
        "    \"\"\"\n",
        "    print(\"---RESEARCHER (with SerpApi Web Search)---\")\n",
        "    topic = state[\"topic\"]\n",
        "\n",
        "    # Use the tool to search for information. The .run() method takes the query string.\n",
        "    search_results = search_tool.run(topic)\n",
        "\n",
        "    print(f\"Search Results:\\n{search_results}\")\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"\"\"You are a helpful research assistant. Based on the following search results,\n",
        "        provide a brief, easy-to-understand explanation of the topic: {topic}.\n",
        "\n",
        "        Search Results:\n",
        "        {search_results}\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    chain = prompt | llm\n",
        "    result = chain.invoke({\"topic\": topic, \"search_results\": search_results})\n",
        "\n",
        "    print(f\"Researcher's Explanation:\\n{result.content}\")\n",
        "    return {\"explanation\": result.content}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d137f518",
      "metadata": {
        "id": "d137f518"
      },
      "outputs": [],
      "source": [
        "def summarizer_agent(state: ResearchState) -> dict:\n",
        "    \"\"\"\n",
        "    This agent takes an explanation and summarizes it in one sentence.\n",
        "    \"\"\"\n",
        "    print(\"---SUMMARIZER---\")\n",
        "    explanation = state[\"explanation\"]\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"You are a summarization expert. Condense the following text into a single, concise sentence:\\n\\n{explanation}\"\n",
        "    )\n",
        "\n",
        "    chain = prompt | llm\n",
        "    result = chain.invoke({\"explanation\": explanation})\n",
        "\n",
        "    print(f\"Summarizer's Output:\\n{result.content}\")\n",
        "    return {\"summary\": result.content}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "67d37079",
      "metadata": {
        "id": "67d37079"
      },
      "outputs": [],
      "source": [
        "# --- Graph Definition ---\n",
        "\n",
        "workflow = StateGraph(ResearchState)\n",
        "\n",
        "# Add the agent functions as nodes\n",
        "workflow.add_node(\"researcher\", researcher_agent)\n",
        "workflow.add_node(\"summarizer\", summarizer_agent)\n",
        "\n",
        "# Define the edges, which control the flow\n",
        "workflow.add_edge(\"researcher\", \"summarizer\")\n",
        "workflow.add_edge(\"summarizer\", END)\n",
        "\n",
        "# Set the entry point\n",
        "workflow.set_entry_point(\"researcher\")\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "adbe03ed",
      "metadata": {
        "id": "adbe03ed",
        "outputId": "e3d53843-885d-4a82-ca69-c6abba6ae919",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter a topic for the agents to research and summarize: hi this is karthik do u know about llmks\n",
            "---RESEARCHER (with SerpApi Web Search)---\n",
            "Search Results:\n",
            "['Title: Hands-on Masterclass on LLMs and ChatGPT Instructor: Dr. Karthik Mohan, UW ECE Affiliate Professor Dates: November 11/12 and 18/19 ...', 'For the last two years, that new technology has been large language models (LLMs): systems that can generate essays, write code, draft emails, ...', \"The reason why LLMs aren't natively very good at data analysis is that they are an average of the data analysis literature available on the internet.\", 'LLMs are revolutionizing work and creativity at an incredible pace! Karthik Harnessing their full potential‚Äîthrough prompt engineering and ...', \"The perspective that I'm coming at to this topic is to consider whether llms actually understand any of the human conversation data that they process.\", 'Curious about the limits of #LLMs in planning and reasoning? Discover how something as simple as stacking blocks can halt the #LLMArmageddon!', 'In this webinar we will discuss the latest advancements in uh large language models or LLMs and the recent trend of agent AI agents.', \"On the role of Large Language Models in Planning (Karthik Valmeekam's PhD Proposal) ... (How) Do LLMs Reason? (Talk given at MILA/ChandarLab).\", 'LLMs Are Useless Without This ‚Äì Prompt Evaluations Explained Large Language Models (LLMs) are powerful ‚Äî but without proper prompt ...', 'This video explains in Tamil, how to fine tune a Tamil Story telling LLM model. This is part 2 of the fine tuning LLM series.']\n",
            "Researcher's Explanation:\n",
            "Based on the search results, here's a simple explanation:\n",
            "\n",
            "**\"LLMks\"** likely refers to **Large Language Models (LLMs)** in the context of **Dr. Karthik Mohan**, an instructor at the University of Washington who teaches masterclasses on LLMs and AI like ChatGPT. Here‚Äôs a quick breakdown:\n",
            "\n",
            "1. **What are LLMs?**  \n",
            "   Large Language Models (e.g., ChatGPT) are AI systems trained on vast amounts of text. They can generate human-like text, write code, answer questions, or draft emails.  \n",
            "\n",
            "2. **Karthik‚Äôs Role**  \n",
            "   Dr. Karthik Mohan is an expert who teaches courses about LLMs‚Äîcovering how they work, their applications (e.g., creativity, data analysis), and their limitations (e.g., struggles with reasoning, planning, or nuanced tasks).  \n",
            "\n",
            "3. **Key Points from the Search Results**  \n",
            "   - LLMs are powerful but not perfect (e.g., they ‚Äúaverage‚Äù knowledge and can‚Äôt deeply reason).  \n",
            "   - **Prompt engineering** (crafting precise inputs) is vital to get good results.  \n",
            "   - They‚Äôre used in diverse fields, from storytelling (e.g., Tamil-language models) to AI agents.  \n",
            "\n",
            "In short: If you meant **LLMs**, they‚Äôre AI tools that mimic human language, and **Karthik Mohan** is an educator helping people master their use. If you meant something else by \"LLMks,\" feel free to clarify! üòä\n",
            "---SUMMARIZER---\n",
            "Summarizer's Output:\n",
            "Dr. Karthik Mohan, an expert instructor at the University of Washington, teaches masterclasses on **Large Language Models (LLMs)**‚ÄîAI systems like ChatGPT that generate human-like text for diverse applications (e.g., coding, creativity) but face limitations in reasoning and nuance, requiring skilled prompt engineering for optimal use.\n",
            "\n",
            "--- FINAL SUMMARY ---\n",
            "Dr. Karthik Mohan, an expert instructor at the University of Washington, teaches masterclasses on **Large Language Models (LLMs)**‚ÄîAI systems like ChatGPT that generate human-like text for diverse applications (e.g., coding, creativity) but face limitations in reasoning and nuance, requiring skilled prompt engineering for optimal use.\n"
          ]
        }
      ],
      "source": [
        "# --- Main Execution Block ---\n",
        "if __name__ == \"__main__\":\n",
        "    topic = input(\"Please enter a topic for the agents to research and summarize: \")\n",
        "\n",
        "    inputs = {\"topic\": topic}\n",
        "    final_state = app.invoke(inputs)\n",
        "\n",
        "    print(\"\\n--- FINAL SUMMARY ---\")\n",
        "    print(final_state['summary'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fgQJ7JW0ao0J"
      },
      "id": "fgQJ7JW0ao0J",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "agents",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}